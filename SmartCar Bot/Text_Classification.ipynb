{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training BERT for text classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\rohit\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import json"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"Dataset\\Data.json\",\"r\") as f:\n",
    "    data = json.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_inputs = []\n",
    "trigger_labels = []\n",
    "for item in data:\n",
    "    user_inputs.append(item[\"Question\"])\n",
    "    trigger_labels.append(item[\"Trigger\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_to_index = {label: index for index, label in enumerate(set(trigger_labels))}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Engine_On()': 0,\n",
       " 'Rollup_Windows()': 1,\n",
       " 'Get_Charge()': 2,\n",
       " 'Get_System_Status()': 3,\n",
       " 'Unknown()': 4,\n",
       " 'Check_Doors()': 5,\n",
       " 'Get_Range()': 6,\n",
       " 'Unlock_Doors()': 7,\n",
       " 'Check_Windows()': 8,\n",
       " 'Lock_Doors()': 9,\n",
       " 'Rolldown_Windows()': 10,\n",
       " 'Check_Engine()': 11,\n",
       " 'Engine_Off()': 12}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "label_to_index"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "model_name = \"bert-base-uncased\"\n",
    "tokenizer = BertTokenizer.from_pretrained(model_name)\n",
    "num_labels = len(label_to_index)\n",
    "model = BertForSequenceClassification.from_pretrained(model_name, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.to(device)\n",
    "input_ids = []\n",
    "attention_masks = []\n",
    "\n",
    "for text in user_inputs:\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text, \n",
    "        add_special_tokens = True, \n",
    "        max_length=128, \n",
    "        padding = \"max_length\", \n",
    "        truncation=True, \n",
    "        return_attention_mask = True, \n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids.append(encoded_dict[\"input_ids\"])\n",
    "    attention_masks.append(encoded_dict[\"attention_mask\"])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_ids = torch.cat(input_ids, dim =0).to(device)\n",
    "attention_masks = torch.cat(attention_masks, dim=0).to(device)\n",
    "labels = torch.tensor([label_to_index[label] for label in trigger_labels], device=device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 32\n",
    "data_set = TensorDataset(input_ids, attention_masks, labels)\n",
    "data_loader = DataLoader(data_set, batch_size=batch_size, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.AdamW(model.parameters(), lr=2e-5)\n",
    "loss_fn = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25 - Loss: 2.5728\n",
      "Epoch 2/25 - Loss: 2.3205\n",
      "Epoch 3/25 - Loss: 2.2067\n",
      "Epoch 4/25 - Loss: 2.0357\n",
      "Epoch 5/25 - Loss: 1.8914\n",
      "Epoch 6/25 - Loss: 1.7407\n",
      "Epoch 7/25 - Loss: 1.5688\n",
      "Epoch 8/25 - Loss: 1.4052\n",
      "Epoch 9/25 - Loss: 1.2232\n",
      "Epoch 10/25 - Loss: 1.0709\n",
      "Epoch 11/25 - Loss: 0.9667\n",
      "Epoch 12/25 - Loss: 0.8321\n",
      "Epoch 13/25 - Loss: 0.7145\n",
      "Epoch 14/25 - Loss: 0.6028\n",
      "Epoch 15/25 - Loss: 0.5161\n",
      "Epoch 16/25 - Loss: 0.4461\n",
      "Epoch 17/25 - Loss: 0.3963\n",
      "Epoch 18/25 - Loss: 0.3452\n",
      "Epoch 19/25 - Loss: 0.2989\n",
      "Epoch 20/25 - Loss: 0.2746\n",
      "Epoch 21/25 - Loss: 0.2400\n",
      "Epoch 22/25 - Loss: 0.2134\n",
      "Epoch 23/25 - Loss: 0.2023\n",
      "Epoch 24/25 - Loss: 0.1814\n",
      "Epoch 25/25 - Loss: 0.1760\n"
     ]
    }
   ],
   "source": [
    "for epoch in range(num_epochs):\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "\n",
    "    for batch in data_loader:\n",
    "        optimizer.zero_grad()\n",
    "        batch_input_ids, batch_attention_masks, batch_labels = batch\n",
    "        output = model(input_ids=batch_input_ids, attention_mask=batch_attention_masks, labels=batch_labels)\n",
    "        loss = output.loss\n",
    "        total_loss += loss.item()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "    \n",
    "    avg_loss = total_loss / len(data_loader)\n",
    "    print(f\"Epoch {epoch + 1}/{num_epochs} - Loss: {avg_loss:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./trained_Classification_model/tokenizer_config.json',\n",
       " './trained_Classification_model/special_tokens_map.json',\n",
       " './trained_Classification_model/vocab.txt',\n",
       " './trained_Classification_model/added_tokens.json')"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_dir = \"./trained_Classification_model/\"\n",
    "model.save_pretrained(output_dir)\n",
    "tokenizer.save_pretrained(output_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing the model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "\n",
    "# Load the trained model and tokenizer\n",
    "output_dir = \"./trained_Classification_model/\"\n",
    "model = BertForSequenceClassification.from_pretrained(output_dir)\n",
    "tokenizer = BertTokenizer.from_pretrained(output_dir)\n",
    "\n",
    "# Function to predict the trigger label for user input text\n",
    "def predict_trigger(text):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        text,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\",\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_dict[\"input_ids\"]\n",
    "    attention_mask = encoded_dict[\"attention_mask\"]\n",
    "\n",
    "    # Perform inference\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "\n",
    "    logits = output.logits\n",
    "    predicted_label_idx = torch.argmax(logits, dim=1).item()\n",
    "\n",
    "    # Map the predicted index back to the actual trigger label\n",
    "    index_to_label = {index: label for label, index in label_to_index.items()}\n",
    "    predicted_label = index_to_label[predicted_label_idx]\n",
    "\n",
    "    return predicted_label\n",
    "\n",
    "# Example usage:\n",
    "user_input = \"Can you lock the car doors please?\"\n",
    "predicted_trigger = predict_trigger(user_input)\n",
    "print(\"Predicted Trigger:\", predicted_trigger)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Trigger: Check_Doors()\n",
      "Predicted Trigger: Unlock_Doors()\n",
      "Predicted Trigger: Engine_On()\n",
      "Predicted Trigger: Unlock_Doors()\n",
      "Predicted Trigger: Get_Charge()\n",
      "Predicted Trigger: Check_Doors()\n",
      "Predicted Trigger: Unlock_Doors()\n",
      "Predicted Trigger: Check_Doors()\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from functions import functions\n",
    "def load_model_and_tokenizer(model_dir):\n",
    "    model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "    return model, tokenizer\n",
    "\n",
    "def classify_input(user_input, model, tokenizer, label_to_index):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        user_input,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "    input_ids = encoded_dict[\"input_ids\"]\n",
    "    attention_mask = encoded_dict[\"attention_mask\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = output.logits\n",
    "    \n",
    "    predicted_label_index = torch.argmax(logits).item()\n",
    "    predicted_label = [label for label, index in label_to_index.items() if index == predicted_label_index][0]\n",
    "    \n",
    "    return predicted_label\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_dir = \"./trained_Classification_model/\"\n",
    "    label_to_index = {'Engine_On()': 0,\n",
    " 'Rollup_Windows()': 1,\n",
    " 'Get_Charge()': 2,\n",
    " 'Get_System_Status()': 3,\n",
    " 'Unknown()': 4,\n",
    " 'Check_Doors()': 5,\n",
    " 'Get_Range()': 6,\n",
    " 'Unlock_Doors()': 7,\n",
    " 'Check_Windows()': 8,\n",
    " 'Lock_Doors()': 9,\n",
    " 'Rolldown_Windows()': 10,\n",
    " 'Check_Engine()': 11,\n",
    " 'Engine_Off()': 12}\n",
    "    \n",
    "    model, tokenizer = load_model_and_tokenizer(model_dir)\n",
    "    \n",
    "    while True:\n",
    "        user_input = input(\"Enter a sentence: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        \n",
    "        predicted_trigger = classify_input(user_input, model, tokenizer, label_to_index)\n",
    "        print(f\"Predicted Trigger: {predicted_trigger}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# this one works better\n",
    "import json\n",
    "import torch\n",
    "from transformers import BertTokenizer, BertForSequenceClassification\n",
    "from functions import functions\n",
    "def load_model_and_tokenizer(model_dir):\n",
    "    model = BertForSequenceClassification.from_pretrained(model_dir)\n",
    "    tokenizer = BertTokenizer.from_pretrained(model_dir)\n",
    "    return model, tokenizer\n",
    "\n",
    "def classify_input(user_input, model, tokenizer, data):\n",
    "    encoded_dict = tokenizer.encode_plus(\n",
    "        user_input,\n",
    "        add_special_tokens=True,\n",
    "        max_length=128,\n",
    "        padding=\"max_length\",\n",
    "        truncation=True,\n",
    "        return_attention_mask=True,\n",
    "        return_tensors=\"pt\"\n",
    "    )\n",
    "\n",
    "    input_ids = encoded_dict[\"input_ids\"]\n",
    "    attention_mask = encoded_dict[\"attention_mask\"]\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        output = model(input_ids=input_ids, attention_mask=attention_mask)\n",
    "        logits = output.logits\n",
    "    \n",
    "    predicted_label_index = str(torch.argmax(logits).item())\n",
    "    #print(type(predicted_label_index), predicted_label_index)\n",
    "    function = data[predicted_label_index]\n",
    "    functions.getting_function(value=function)\n",
    "    #print(\"*\",function, type(function))\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    model_dir = \"./trained_Classification_model/\"    \n",
    "    model, tokenizer = load_model_and_tokenizer(model_dir)\n",
    "    with open(\"Label_to_Function.json\", \"r\") as mappings:\n",
    "        data = json.load(mappings)\n",
    "    while True:\n",
    "        user_input = input(\"Enter a sentence: \")\n",
    "        if user_input.lower() == \"exit\":\n",
    "            break\n",
    "        classify_input(user_input, model, tokenizer, data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.2"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
